{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAYESIAN MACHINE LEARNING - PROJECT\n",
    "# Practical Bayesian Optimization of Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True, floatmode='fixed')\n",
    "import numpy.random as npr\n",
    "import scipy.stats as sps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "%matplotlib notebook\n",
    "\n",
    "from functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D example\n",
    "\n",
    "An example with a simple target function and one parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example\n",
    "def target(x, wait=True):\n",
    "    if wait: time.sleep(np.abs(5*x[0]*np.sin(x[0])))\n",
    "    return -x**2 * np.sin(5 * np.pi * x)**6.0   # target function\n",
    "\n",
    "space = np.array([[0, 1]])   # parameters space\n",
    "nu = 0.1   # noise standard deviation\n",
    "N0 = 5   # number of random initial evaluations\n",
    "N = 30   # maximum number of evaluations\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by generating some data and doing the optimization with the classical EI criterion as acquisition function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = generate_data(target, nu, N0, space, seed)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(2, figsize=(10, 8))\n",
    "# Plot training data\n",
    "ax[0].plot(X, y, 'ro', label=\"initial train set\")\n",
    "\n",
    "# Plot target function on top\n",
    "xPlot = np.linspace(space[0, 0], space[0, 1], 201)\n",
    "ax[0].plot(xPlot, target(xPlot, wait=False), 'b--', label=\"target\", linewidth=2)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlim(space[0, 0], space[0, 1])\n",
    "\n",
    "boEI = BayesianOptimizationEI(X, y, space, kernel=1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5) + 1.0 * WhiteKernel(noise_level=.1))\n",
    "boEI.fit() # Fit the GP to the input data\n",
    "\n",
    "# Visualize the posterior by showing samples on top of the training data above.\n",
    "number_of_samples = 10\n",
    "for Seed in range(number_of_samples):\n",
    "    yPlot = boEI.sample_y(xPlot.reshape(-1, 1), random_state=Seed).flatten()\n",
    "    ax[0].plot(xPlot, yPlot, alpha=.3, zorder=1)\n",
    "plt.show()\n",
    "\n",
    "ax[1].plot(xPlot, boEI.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlim(space[0, 0], space[0, 1])\n",
    "\n",
    "#fig.savefig('toy_EI_0.eps', format='eps', bbox_inches='tight', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "begin = time.time()\n",
    "# optimization\n",
    "while boEI.X.shape[0] < N:\n",
    "    # perform one step of BO\n",
    "    x_next = boEI.find_next_point(number_of_restarts=10)\n",
    "    y_next = eval_objective(target, x_next.reshape(1, space.shape[0]), nu=nu)\n",
    "    print('Iteration {:3d}: x_next = {:1.3f}, y_next = {:1.3f}'.format(boEI.X.shape[0]+1, x_next[0], y_next[0, 0]))\n",
    "    boEI.update(x_next, y_next)\n",
    "    boEI.fit()\n",
    "\n",
    "    # plot the corresponding sample\n",
    "    ax[0].plot(x_next, y_next[0], marker='$'+str(boEI.X.shape[0])+'$', color='r')\n",
    "    ax[1].clear()\n",
    "    ax[1].plot(xPlot, boEI.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlim(space[0, 0], space[0, 1])\n",
    "    #fig.savefig('toy_EI_' + str(boEI.X.shape[0]) + '.eps', format='eps', bbox_inches='tight', dpi=1200)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "i_best = np.argmin(boEI.y)\n",
    "x_best, y_best = boEI.X[i_best], boEI.y[i_best]\n",
    "print('x_best = {:1.3f}, y_best = {:1.3f}'.format(x_best[0], y_best[0]))\n",
    "print('Time: ', end-begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we process similarly with the EI per seconds criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, t = generate_data(target, nu, N0, space, seed=1, countTime=True)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(3, figsize=(10, 8))\n",
    "# Plot training data\n",
    "ax[0].plot(X, y, 'ro', label=\"initial train set\")\n",
    "ax[1].plot(X, t, 'ro', label=\"train time\")\n",
    "\n",
    "# Plot target function on top\n",
    "xPlot = np.linspace(space[0, 0], space[0, 1], 201)\n",
    "ax[0].plot(xPlot, target(xPlot, wait=False), 'b--', label=\"target\", linewidth=2)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlim(space[0, 0], space[0, 1])\n",
    "ax[1].legend()\n",
    "ax[1].set_xlim(space[0, 0], space[0, 1])\n",
    "\n",
    "boEIperS = BayesianOptimizationEIperS(X, y, t, space, kernel=1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5) + 1.0 * WhiteKernel(noise_level=.1))\n",
    "boEIperS.fit() # Fit the GP to the input data\n",
    "\n",
    "# Visualize the posterior by showing samples on top of the training data above.\n",
    "number_of_samples = 10\n",
    "for seed in range(number_of_samples):\n",
    "    yPlot = boEIperS.sample_y(xPlot.reshape(-1, 1), random_state=seed).flatten()\n",
    "    ax[0].plot(xPlot, yPlot, alpha=.3, zorder=1)\n",
    "plt.show()\n",
    "\n",
    "ax[2].plot(xPlot, boEIperS.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI/s')\n",
    "ax[2].legend()\n",
    "ax[2].set_xlim(space[0, 0], space[0, 1])\n",
    "\n",
    "#fig.savefig('toy_EIperS_0.eps', format='eps', bbox_inches='tight', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "begin = time.time()\n",
    "# optimization\n",
    "while boEIperS.X.shape[0] < N:\n",
    "    # perform one step of BO\n",
    "    x_next = boEIperS.find_next_point(number_of_restarts=10)\n",
    "    y_next, t_next = eval_objective(target, x_next.reshape(1, space.shape[0]), nu=nu, countTime=True)\n",
    "    print('Iteration {:3d}: x_next = {:1.3f}, y_next = {:1.3f}'.format(boEIperS.X.shape[0]+1, x_next[0], y_next[0, 0]))\n",
    "    boEIperS.update(x_next, y_next, t_next)\n",
    "    boEIperS.fit()\n",
    "\n",
    "    # plot the corresponding sample\n",
    "    ax[0].plot(x_next, y_next[0], marker='$'+str(boEIperS.X.shape[0])+'$', color='r')\n",
    "    ax[1].plot(x_next, t_next[0], marker='$'+str(boEIperS.X.shape[0])+'$', color='r')\n",
    "    ax[2].clear()\n",
    "    ax[2].plot(xPlot, boEIperS.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI/s')\n",
    "    ax[2].legend()\n",
    "    ax[2].set_xlim(space[0, 0], space[0, 1])\n",
    "    #fig.savefig('toy_EIperS_' + str(boEIperS.X.shape[0]) + '.eps', format='eps', bbox_inches='tight', dpi=1200)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "i_best = np.argmin(boEIperS.y)\n",
    "x_best, y_best = boEIperS.X[i_best], boEIperS.y[i_best]\n",
    "print('x_best = {:1.3f}, y_best = {:1.3f}'.format(x_best[0], y_best[0]))\n",
    "print('Time: ', end-begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to the optimization of Machine Learning Algorithms hyperparameters\n",
    "\n",
    "We consider different machine learning algorithms. First we define some loss (ie target functions), one with one hyperparameter, the other with two parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def kernel_Ridge_loss(parameters):\n",
    "    #We consider kernel Ridge regression with one parameter that we want to optimizeng='mean_squared_error').mean()\n",
    "    \n",
    "    R = KernelRidge(alpha=10**parameters[0], kernel='rbf', gamma=1)\n",
    "    R.fit(x_train, y_train)\n",
    "    \n",
    "    score = mean_squared_error(y_train, R.predict(x_train))\n",
    "    \n",
    "    return score\n",
    "\n",
    "def SVM_loss(parameters):\n",
    "    #We consider cross valuation with two parameters that we want to optimize\n",
    "    score = cross_val_score(SVC(C=10**parameters[0], gamma=10**parameters[1]), X=x_train, y=y_train).mean()    \n",
    "    return score\n",
    "\n",
    "# feel free to use another loss:\n",
    "#def loss(parameters):\n",
    "#    \n",
    "#    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select the Machine Learning problem we want to optimize by running one of the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iris dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "title = 'SVM_Iris'\n",
    "\n",
    "x_train, y_train = load_iris(True)\n",
    "\n",
    "# parameters\n",
    "space = np.array([[-2, 5], [-6, 0]], dtype='int')\n",
    "nu = 0\n",
    "N0 = 5\n",
    "seed = 1\n",
    "N = 50\n",
    "target = SVM_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Sonar dataset, you can download the data at https://datahub.io/machine-learning/sonar\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "title = 'SVM_Sonar'\n",
    "\n",
    "data = pd.read_csv('sonar.csv', sep=',', header=0).values\n",
    "\n",
    "#data = np.genfromtxt('sonar.csv', delimiter=',')\n",
    "x_train = data[:, 0:-1].astype('float')\n",
    "y_train = np.zeros(x_train.shape[0], dtype='int')\n",
    "y_train[data[:, -1] == 'Rock'] = 1\n",
    "\n",
    "space = np.array([[-2, 5], [-6, 0]], dtype='int')\n",
    "nu = 0\n",
    "N0 = 5\n",
    "seed = 1\n",
    "N = 200\n",
    "target = SVM_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Motorcycle data, regression, available at http://www.tsi.enst.fr/~roueff/edu/sd205/Motorcycledata.txt\n",
    "\n",
    "title = 'SVM_Motorcycle'\n",
    "\n",
    "data = np.loadtxt('Motorcycledata.txt')\n",
    "x_train = data[:, 1].reshape(-1, 1)\n",
    "y_train = data[:, -1].astype('int')\n",
    "\n",
    "space = np.array([[-2, 5], [-6, 0]], dtype='int')\n",
    "nu = 0\n",
    "N0 = 5\n",
    "seed = 1\n",
    "N = 50\n",
    "target = SVM_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HTRU2 dataset, available at https://archive.ics.uci.edu/ml/datasets/HTRU2. \n",
    "#Evaluations are quite long with this dataset. You can use only a small part of the data\n",
    "\n",
    "title = 'SVM_HTRU2'\n",
    "\n",
    "data = np.genfromtxt('HTRU_2.csv', delimiter=',')\n",
    "x_train = data[:, 0:-1] \n",
    "y_train = data[:, -1].astype('int64')\n",
    "\n",
    "space = np.array([[-2, 5], [-6, 0]], dtype='int')\n",
    "nu = 0\n",
    "N0 = 5\n",
    "seed = 1\n",
    "N = 50\n",
    "target = SVM_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Motorcycle data, regression, available at http://www.tsi.enst.fr/~roueff/edu/sd205/Motorcycledata.txt\n",
    "\n",
    "title = 'kernel_Ridge_Motorcycle'\n",
    "\n",
    "data = np.loadtxt('Motorcycledata.txt')\n",
    "x_train = data[:, 1].reshape(-1, 1)\n",
    "y_train = data[:, -1].reshape(-1, 1)\n",
    "\n",
    "space = np.array([[-2, 2]], dtype='int')\n",
    "xPlot = np.linspace(space[0, 0], space[0, 1], 201)\n",
    "nu = 0\n",
    "N0 = 5\n",
    "seed = 1\n",
    "N = 50\n",
    "target = kernel_Ridge_loss\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feel free to use another dataset: \n",
    "\n",
    "x_train = \n",
    "y_train = \n",
    "\n",
    "space = np.array([], dtype='int') # array of shape (dim, 2)\n",
    "nu = \n",
    "N0 = \n",
    "seed = 1\n",
    "N = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# specify the kernels to compare\n",
    "lst_kernels = [Matern(nu=2.5), Matern(nu=1.5), Matern(nu=0.5), RBF(), Matern(nu=2.5) + WhiteKernel()]\n",
    "lst_labels = ['Matern(2,5)', 'Matern(1,5)', 'Absolute exponential', 'RBF', 'Matern(2,5) + White']\n",
    "plot = True\n",
    "\n",
    "nb_kernels = len(lst_kernels)\n",
    "\n",
    "lst_y_best = np.zeros((nb_kernels, N))\n",
    "\n",
    "X, y = generate_data(target, nu, N0, space, 2)\n",
    "print('Initialisation with {} evaluations'.format(N0))\n",
    "\n",
    "for (k, (kernel, label)) in enumerate(zip(lst_kernels, lst_labels)):\n",
    "    print('Kernel = ' + label)\n",
    "    \n",
    "    boEI = BayesianOptimizationEI(X, y, space, kernel=kernel)\n",
    "    boEI.fit()\n",
    "\n",
    "    if space.shape[0] == 1 and plot:\n",
    "        fig, ax = plt.subplots(2, figsize=(10, 8))\n",
    "        ax[0].plot(X, y, 'ro', label=\"initial train set\")\n",
    "        ax[0].set_xlim(space[0, 0], space[0, 1])\n",
    "        ax[0].legend()\n",
    "\n",
    "    while boEI.X.shape[0] < N:\n",
    "        # perform one step of BO\n",
    "        x_next = boEI.find_next_point(number_of_restarts=3) # returns the maximum of your acquisition criterion\n",
    "        y_next = eval_objective(target, x_next.reshape(1, space.shape[0]), nu=nu)\n",
    "        if (boEI.X.shape[0]+1) % 10 == 0:\n",
    "            print('Iteration {:3d}: x_next = {}, y_next = {:1.3f}'.format(boEI.X.shape[0]+1, x_next, y_next[0, 0]))\n",
    "        boEI.update(x_next, y_next)\n",
    "        boEI.fit()\n",
    "\n",
    "        # plot the corresponding sample\n",
    "        if space.shape[0] == 1 and plot:\n",
    "            ax[0].plot(x_next, y_next[0], marker='$'+str(boEI.X.shape[0])+'$', color='r')\n",
    "            ax[1].clear()\n",
    "            ax[1].plot(xPlot, boEI.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI')\n",
    "    \n",
    "    lst_y_best[k] = np.minimum.accumulate(boEI.y.reshape(N))\n",
    "\n",
    "print(lst_y_best[:, [0, N//4, N//2, 3*N//4, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for (k, (kernel, label)) in enumerate(zip(lst_kernels, lst_labels)):\n",
    "    plt.plot(np.arange(1, N+1), lst_y_best[k], label=label)\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Best value')\n",
    "plt.title('Kernel comparison')\n",
    "\n",
    "# plt.savefig('KernelComparison_' + title + '_.eps', format='eps', bbox_inches='tight', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between EI and EI per second\n",
    "\n",
    "Let us execute the simple Bayesian Optimization with EI as acquisition criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "X, y, t = generate_data(target, nu, N0, space, seed, countTime=True)\n",
    "print('Initialisation with {} evaluations'.format(N0))\n",
    "\n",
    "boEI = BayesianOptimizationEI(X, y, space, kernel=Matern(nu=2.5))\n",
    "boEI.fit() # Fit the GP to the input data\n",
    "\n",
    "timesEI = list(np.cumsum(t.reshape(N0)))\n",
    "\n",
    "if space.shape[0] == 1:\n",
    "    fig, ax = plt.subplots(2, figsize=(10, 8))\n",
    "    xPlot = np.linspace(space[0, 0], space[0, 1], 100)\n",
    "    ax[0].plot(X, y, 'ro', label=\"initial train set\")\n",
    "    ax[0].set_xlim(space[0, 0], space[0, 1])\n",
    "    ax[0].legend()\n",
    "\n",
    "while boEI.X.shape[0] < N:\n",
    "    # perform one step of BO\n",
    "    x_next = boEI.find_next_point(number_of_restarts=3)\n",
    "    y_next = eval_objective(target, x_next.reshape(1, space.shape[0]), nu=nu)\n",
    "    if (boEI.X.shape[0]+1) % 10 == 0:\n",
    "        print('Iteration {:3d}: x_next = {}, y_next = {:1.3f}'.format(boEI.X.shape[0]+1, x_next, y_next[0, 0]))\n",
    "    boEI.update(x_next, y_next)\n",
    "    boEI.fit()\n",
    "    timesEI.append(time.time()-time0)\n",
    "\n",
    "    # plot the corresponding sample\n",
    "    if space.shape[0] == 1:\n",
    "        ax[0].plot(x_next, y_next[0], marker='$'+str(boEI.X.shape[0])+'$', color='r')\n",
    "        ax[1].clear()\n",
    "        ax[1].plot(xPlot, boEI.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI')\n",
    "\n",
    "lst_y_best_EI = np.minimum.accumulate(boEI.y.reshape(N))\n",
    "\n",
    "#print(lst_y_best[:, [0, N//4, N//2, 3*N//4, -1]])\n",
    "\n",
    "i_best = np.argmin(boEI.y)\n",
    "x_best, y_best = boEI.X[i_best], boEI.y[i_best]\n",
    "print('x_best = {}, y_best = {:1.3f}'.format(x_best, y_best[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the same with EI per second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "X, y, t = generate_data(target, nu, N0, space, seed, countTime=True)\n",
    "print('Initialisation with {} evaluations'.format(N0))\n",
    "\n",
    "boEIperS = BayesianOptimizationEIperS(X, y, t, space, kernel=Matern(nu=2.5))\n",
    "boEIperS.fit() # Fit the GP to the input data\n",
    "\n",
    "timesEIperS = list(np.cumsum(t.reshape(N0)))\n",
    "\n",
    "if space.shape[0] == 1:\n",
    "    fig, ax = plt.subplots(3, figsize=(10, 8))\n",
    "    xPlot = np.linspace(space[0, 0], space[0, 1], 100)\n",
    "    ax[0].plot(X, y, 'ro', label=\"initial train set\")\n",
    "    ax[0].set_xlim(space[0, 0], space[0, 1])\n",
    "    ax[0].legend()\n",
    "    \n",
    "    ax[1].plot(X, t, 'ro', label=\"time\")\n",
    "    ax[1].set_xlim(space[0, 0], space[0, 1])\n",
    "    ax[1].legend()\n",
    "\n",
    "while boEIperS.X.shape[0] < N:\n",
    "    # perform one step of BO\n",
    "    x_next = boEIperS.find_next_point(number_of_restarts=3) # returns the maximum of your acquisition criterion\n",
    "    y_next, t_next = eval_objective(target, x_next.reshape(1, space.shape[0]), nu=nu, countTime=True)\n",
    "    if (boEIperS.X.shape[0]+1) % 10 == 0:\n",
    "        print('Iteration {:3d}: x_next = {}, y_next = {:1.3f}, t_next = {:1.3f}'.format(boEIperS.X.shape[0]+1, x_next, y_next[0, 0], t_next[0, 0]))\n",
    "    boEIperS.update(x_next, y_next, t_next)\n",
    "    boEIperS.fit()\n",
    "    timesEIperS.append(time.time()-time0)\n",
    "\n",
    "    # plot the corresponding sample\n",
    "    if space.shape[0] == 1:\n",
    "        ax[0].plot(x_next, y_next[0], marker='$'+str(boEIperS.X.shape[0])+'$', color='r')\n",
    "        ax[1].plot(x_next, t_next[0], marker='$'+str(boEIperS.X.shape[0])+'$', color='r')\n",
    "        ax[2].clear()\n",
    "        ax[2].plot(xPlot, boEIperS.acquisition_criterion(xPlot.reshape(-1, 1)), label='EI')\n",
    "\n",
    "lst_y_best_EIperS = np.minimum.accumulate(boEIperS.y.reshape(N))\n",
    "\n",
    "i_best = np.argmin(boEIperS.y)\n",
    "x_best, y_best = boEIperS.X[i_best], boEIperS.y[i_best]\n",
    "print('x_best = {}, y_best = {:1.3f}'.format(x_best, y_best[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(1, N+1), lst_y_best_EI, label='EI')\n",
    "plt.plot(np.arange(1, N+1), lst_y_best_EIperS, label='EI/s')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of evaluations')\n",
    "plt.ylabel('Best value')\n",
    "plt.title('Optimization comparison')\n",
    "\n",
    "#plt.savefig('Optimization_Comparison_Evaluations_' + title + '_.eps', format='eps', bbox_inches='tight', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(timesEI, lst_y_best_EI, label='EI')\n",
    "plt.step(timesEIperS, lst_y_best_EIperS, label='EI/s')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Best value')\n",
    "plt.title('Optimization comparison')\n",
    "\n",
    "#plt.savefig('Optimization_Comparison_Time_' + title + '_.eps', format='eps', bbox_inches='tight', dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
